{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"logRegProj\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"customer_churn.csv\", inferSchema=True, \n",
    "                    header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Names',\n",
       " 'Age',\n",
       " 'Total_Purchase',\n",
       " 'Account_Manager',\n",
       " 'Years',\n",
       " 'Num_Sites',\n",
       " 'Onboard_date',\n",
       " 'Location',\n",
       " 'Company',\n",
       " 'Churn']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which columns are important for predicting churn? \n",
    "#### We are given that the \"Account_Manager\" column is assigned at random and hence we should not consider it. We can also ignore the  \"Names\" and \"Onboard_date\" columns since logically they should not matter. If we think that \"Onbard_date\" tells us the amount of time the customer is present, we can obtain that information via the \"Years\" column instead. We can now study if the \"Location\" column is important. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is \"Location\" important? How many unique locations? How many total locations? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique locations = 900\n",
      "Number of records = 900\n"
     ]
    }
   ],
   "source": [
    "location_df = df.select([\"Location\"]).toPandas()\n",
    "print(\"Number of unique locations = {}\".format(len(location_df[\"Location\"].unique())))\n",
    "print(\"Number of records = {}\".format(len(location_df[\"Location\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Could the zip code be a more interesting feature? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract zipcode using the udf function of pyspark.sql. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique locations = 899\n",
      "Number of records = 900\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "def f(my_string):\n",
    "    return my_string.split()[-1]\n",
    "udf_my_function = udf(f, StringType())\n",
    "zipcode_df = df.withColumn(\"Zipcode\", udf_my_function(\"Location\")).select(\"Zipcode\").toPandas()\n",
    "print(\"Number of unique locations = {}\".format(len(zipcode_df[\"Zipcode\"].unique())))\n",
    "print(\"Number of records = {}\".format(len(zipcode_df[\"Zipcode\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the above analysis it is clear that the zipcode and/or location is not correlated with churn as almost all locations (whether considering the address or zipcode) are unique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is \"Company\" important? How many unique companies? How many total companies? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|       Company|count|\n",
      "+--------------+-----+\n",
      "|Anderson Group|    4|\n",
      "|    Wilson PLC|    3|\n",
      "|  Williams PLC|    3|\n",
      "|     Smith Inc|    2|\n",
      "|     Ortiz Ltd|    2|\n",
      "|      King LLC|    2|\n",
      "|     Evans LLC|    2|\n",
      "|      Rice PLC|    2|\n",
      "|    Nelson LLC|    2|\n",
      "|    Walker Ltd|    2|\n",
      "|      Soto PLC|    2|\n",
      "|Davis and Sons|    2|\n",
      "|     Smith Ltd|    2|\n",
      "|  Williams LLC|    2|\n",
      "|      Webb PLC|    2|\n",
      "|   Smith Group|    2|\n",
      "|Smith and Sons|    2|\n",
      "|   Davis Group|    2|\n",
      "|     Jones LLC|    2|\n",
      "|     Gates Ltd|    2|\n",
      "+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "company_df = df.groupBy(\"Company\").count().sort(col(\"count\").desc())\n",
    "company_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "873"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total unique companies\n",
    "len(company_df.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above shows that for most data points the \"Company\" feature is unique and hence less informative. This is a handwavy explanation but we will keep things simpler for now. If we want to consider the company categorical feature, we could use StringIndexer and OneHotEncoder to convert it to a vector. For now, this is commented out. \n",
    "\n",
    "\n",
    "### Thus we will consider the columns -- \"Age\", \"Total_Purchase\", \"Years\", \"Num_Sites\" as our features column and \"Churn\" as our label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_final_data = df.select([\"Age\", \"Total_Purchase\", \"Years\", \"Num_Sites\", \"Churn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+-----+---------+-----+\n",
      "| Age|Total_Purchase|Years|Num_Sites|Churn|\n",
      "+----+--------------+-----+---------+-----+\n",
      "|42.0|       11066.8| 7.22|      8.0|    1|\n",
      "|41.0|      11916.22|  6.5|     11.0|    1|\n",
      "|38.0|      12884.75| 6.67|     12.0|    1|\n",
      "|42.0|       8010.76| 6.71|     10.0|    1|\n",
      "|37.0|       9191.58| 5.56|      9.0|    1|\n",
      "|48.0|      10356.02| 5.12|      8.0|    1|\n",
      "|44.0|      11331.58| 5.23|     11.0|    1|\n",
      "|32.0|       9885.12| 6.92|      9.0|    1|\n",
      "|43.0|       14062.6| 5.46|     11.0|    1|\n",
      "|40.0|       8066.94| 7.11|     11.0|    1|\n",
      "|30.0|      11575.37| 5.22|      8.0|    1|\n",
      "|45.0|       8771.02| 6.64|     11.0|    1|\n",
      "|45.0|       8988.67| 4.84|     11.0|    1|\n",
      "|40.0|       8283.32|  5.1|     13.0|    1|\n",
      "|41.0|       6569.87|  4.3|     11.0|    1|\n",
      "|38.0|      10494.82| 6.81|     12.0|    1|\n",
      "|45.0|       8213.41| 7.35|     11.0|    1|\n",
      "|43.0|      11226.88| 8.08|     12.0|    1|\n",
      "|53.0|       5515.09| 6.85|      8.0|    1|\n",
      "|46.0|        8046.4| 5.69|      8.0|    1|\n",
      "+----+--------------+-----+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_final_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we want to use the \"Company\" as a feature we can convert the \"Company\" column to a one-hot encoded vector since it is categorical data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler, StringIndexer, \n",
    "                                OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company_indexer = StringIndexer(inputCol=\"Company\", outputCol=\"CompanyIndex\")\n",
    "# company_encoder = OneHotEncoder(inputCol=\"CompanyIndex\", outputCol=\"CompanyVec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using assembler and pipeline for model fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembler = VectorAssembler(inputCols=[\"Age\", \"Total_Purchase\", \"Years\", \n",
    "#                                        \"Num_Sites\", \"CompanyVec\"], \n",
    "#                             outputCol=\"features\")\n",
    "assembler = VectorAssembler(inputCols=[\"Age\", \"Total_Purchase\", \"Years\", \n",
    "                                       \"Num_Sites\"], \n",
    "                            outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_churn = LogisticRegression(featuresCol=\"features\", labelCol=\"Churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline(stages=[company_indexer, company_encoder,\n",
    "#                            assembler, log_reg_churn])\n",
    "pipeline = Pipeline(stages=[assembler, log_reg_churn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform train and test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we check if \"Churn\" column is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Churn|count|\n",
      "+-----+-----+\n",
      "|    1|  150|\n",
      "|    0|  750|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_final_data.groupBy(\"Churn\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above data shows that we have most of the datapoints have churn=0. Thus if we split the data into train and test sets randomly, it may happen that all of the churn=1 datapoints remain in one or the other set resulting in a poor test of the trained model. Thus we need to perform stratified splits, i.e, perform 70%-30% split (or any other percentage) per category, i.e., per churn=1 datapoints and separately for churn=0 datapoints. \n",
    "\n",
    "### We now perform stratified splits as shown below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fractions = {0: 0.7, 1: 0.7} denotes that for both class=0 and for class=1 \n",
    "# consider 70% data for the train_data set. \n",
    "train_data = my_final_data.sampleBy(\"Churn\", fractions={0: 0.7, 1: 0.7}, seed=11)\n",
    "# The remaining data goes into test_data set. \n",
    "test_data = my_final_data.subtract(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if the spliting worked as expected: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Churn|count|\n",
      "+-----+-----+\n",
      "|    1|  109|\n",
      "|    0|  523|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|Churn|count|\n",
      "+-----+-----+\n",
      "|    1|   41|\n",
      "|    0|  227|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.groupBy(\"Churn\").count().show()\n",
    "test_data.groupBy(\"Churn\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fitted_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eval = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"Churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|Churn|prediction|\n",
      "+-----+----------+\n",
      "|    0|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    1|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select(\"Churn\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC = my_eval.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8072418609648652"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-spark-practice",
   "language": "python",
   "name": "python-spark-practice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
